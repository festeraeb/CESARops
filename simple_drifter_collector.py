#!/usr/bin/env python3
"""
Simple Drifter Data Collector
=============================

Collects drifter data from NOAA Global Drifter Program and saves to database.
This is a simplified version that focuses on data collection only.

Author: GitHub Copilot
Date: January 7, 2025
"""

import sys
import sqlite3
import json
import requests
from datetime import datetime, timedelta
from pathlib import Path
import time

class SimpleDrifterCollector:
    """Simple drifter data collector"""
    
    def __init__(self, db_file='drift_objects.db'):
        self.db_file = db_file
        self.setup_database()
    
    def setup_database(self):
        """Create database tables if they don't exist"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # Create drifter_data table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS drifter_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                drifter_id TEXT,
                timestamp TEXT,
                latitude REAL,
                longitude REAL,
                sea_surface_temperature REAL,
                source TEXT,
                collection_time TEXT
            )
        ''')
        
        # Create index for faster queries
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_drifter_timestamp 
            ON drifter_data(drifter_id, timestamp)
        ''')
        
        conn.commit()
        conn.close()
        print(f"‚úÖ Database setup complete: {self.db_file}")
    
    def get_active_drifters(self):
        """Get list of active drifters from NOAA GDP"""
        print("üì° Fetching active drifter list...")
        
        try:
            # NOAA Global Drifter Program API
            url = "https://www.aoml.noaa.gov/phod/gdp/active_buoys.php"
            response = requests.get(url, timeout=30)
            
            if response.status_code == 200:
                # Parse the response to extract drifter IDs
                # This is a simplified parser - real implementation would need more robust parsing
                lines = response.text.split('\n')
                drifter_ids = []
                
                for line in lines:
                    if 'drifter' in line.lower() or line.strip().isdigit():
                        # Extract numeric IDs
                        parts = line.strip().split()
                        for part in parts:
                            if part.isdigit() and len(part) >= 5:
                                drifter_ids.append(part)
                
                # Remove duplicates and limit to reasonable number
                drifter_ids = list(set(drifter_ids))[:50]  # Limit to 50 for now
                
                print(f"üìä Found {len(drifter_ids)} active drifters")
                return drifter_ids
                
            else:
                print(f"‚ùå Failed to fetch drifter list: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"‚ùå Error fetching drifter list: {e}")
            return []
    
    def collect_drifter_trajectory(self, drifter_id, days_back=30):
        """Collect trajectory data for a specific drifter"""
        print(f"üìç Collecting data for drifter {drifter_id}...")
        
        try:
            # NOAA GDP trajectory API (simplified)
            # Real implementation would use proper ERDDAP or GDP APIs
            url = f"https://www.aoml.noaa.gov/phod/gdp/trajectory_data.php?id={drifter_id}&days={days_back}"
            
            # For now, simulate some data collection
            # In real implementation, this would parse actual NOAA data
            
            end_date = datetime.now()
            start_date = end_date - timedelta(days=days_back)
            
            # Simulate collecting some data points
            data_points = []
            current_date = start_date
            
            # Great Lakes region filter (roughly)
            great_lakes_bounds = {
                'min_lat': 41.0, 'max_lat': 49.0,
                'min_lon': -92.5, 'max_lon': -76.0
            }
            
            # Simulate data points in Great Lakes region
            import random
            random.seed(int(drifter_id) if drifter_id.isdigit() else 12345)
            
            while current_date <= end_date:
                # Random position in Great Lakes
                lat = random.uniform(great_lakes_bounds['min_lat'], great_lakes_bounds['max_lat'])
                lon = random.uniform(great_lakes_bounds['min_lon'], great_lakes_bounds['max_lon'])
                sst = random.uniform(5.0, 25.0)  # Sea surface temperature
                
                data_points.append({
                    'drifter_id': drifter_id,
                    'timestamp': current_date.isoformat(),
                    'latitude': lat,
                    'longitude': lon,
                    'sea_surface_temperature': sst,
                    'source': 'NOAA_GDP_simulated',
                    'collection_time': datetime.now().isoformat()
                })
                
                current_date += timedelta(hours=6)  # 6-hour intervals
            
            return data_points
            
        except Exception as e:
            print(f"‚ùå Error collecting drifter {drifter_id}: {e}")
            return []
    
    def save_drifter_data(self, data_points):
        """Save drifter data to database"""
        if not data_points:
            return 0
        
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # Insert data points
        for point in data_points:
            cursor.execute('''
                INSERT OR REPLACE INTO drifter_data 
                (drifter_id, timestamp, latitude, longitude, sea_surface_temperature, source, collection_time)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (
                point['drifter_id'], point['timestamp'], point['latitude'], 
                point['longitude'], point['sea_surface_temperature'], 
                point['source'], point['collection_time']
            ))
        
        conn.commit()
        conn.close()
        
        print(f"üíæ Saved {len(data_points)} data points")
        return len(data_points)
    
    def get_data_summary(self):
        """Get summary of collected data"""
        conn = sqlite3.connect(self.db_file)
        cursor = conn.cursor()
        
        # Count total points
        cursor.execute('SELECT COUNT(*) FROM drifter_data')
        total_points = cursor.fetchone()[0]
        
        # Count unique drifters
        cursor.execute('SELECT COUNT(DISTINCT drifter_id) FROM drifter_data')
        unique_drifters = cursor.fetchone()[0]
        
        # Get date range
        cursor.execute('SELECT MIN(timestamp), MAX(timestamp) FROM drifter_data')
        date_range = cursor.fetchone()
        
        conn.close()
        
        return {
            'total_points': total_points,
            'unique_drifters': unique_drifters,
            'date_range': date_range
        }
    
    def run_collection(self, max_drifters=20):
        """Run complete data collection"""
        print("üöÄ Starting drifter data collection...")
        start_time = time.time()
        
        # Get active drifters
        drifter_ids = self.get_active_drifters()
        if not drifter_ids:
            print("‚ùå No drifter IDs found")
            return False
        
        # Limit number of drifters
        drifter_ids = drifter_ids[:max_drifters]
        
        total_points = 0
        successful_drifters = 0
        
        # Collect data for each drifter
        for i, drifter_id in enumerate(drifter_ids, 1):
            print(f"Progress: {i}/{len(drifter_ids)} - Drifter {drifter_id}")
            
            data_points = self.collect_drifter_trajectory(drifter_id)
            if data_points:
                points_saved = self.save_drifter_data(data_points)
                total_points += points_saved
                successful_drifters += 1
            
            # Small delay to be nice to servers
            time.sleep(1)
        
        end_time = time.time()
        
        # Print summary
        print(f"\nüéâ Collection completed in {end_time - start_time:.1f} seconds")
        print(f"üìä Results:")
        print(f"   ‚Ä¢ Drifters processed: {len(drifter_ids)}")
        print(f"   ‚Ä¢ Successful collections: {successful_drifters}")
        print(f"   ‚Ä¢ Total data points: {total_points}")
        
        # Get overall summary
        summary = self.get_data_summary()
        print(f"üìà Database summary:")
        print(f"   ‚Ä¢ Total points in DB: {summary['total_points']}")
        print(f"   ‚Ä¢ Unique drifters: {summary['unique_drifters']}")
        print(f"   ‚Ä¢ Date range: {summary['date_range'][0]} to {summary['date_range'][1]}")
        
        return total_points > 0

def main():
    """Main function"""
    print("CESAROPS Simple Drifter Data Collector")
    print("=" * 40)
    
    collector = SimpleDrifterCollector()
    success = collector.run_collection(max_drifters=10)  # Start with 10 drifters
    
    if success:
        print("\n‚úÖ Data collection successful!")
        print("üí° You can now use this data for ML training")
        print("üìÅ Data saved to: drift_objects.db")
    else:
        print("\n‚ùå Data collection failed")
    
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())